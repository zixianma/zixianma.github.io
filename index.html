<!DOCTYPE html>
<html lang='en'>

<head>
	<meta charset='UTF-8'>
	<meta name='viewport' content='width=device-width, initial-scale=1, shrink-to-fit=no'>
	<title>Zixian Ma</title>
	<meta http-equiv='X-UA-Compatible' content='IE=edge'>
	<link rel='stylesheet' href='asset/css/bootstrap.css'>
	<link rel='stylesheet' href='asset/css/all.css'>
	<link rel='stylesheet' href='asset/css/style.css'>
</head>

<body class='bg-light'>
	<header>
		<nav class='navbar navbar-light fixed-top bg-light'>
			<a class='navbar-brand col-sm-3 col-md-2 mt-1 ml-5' href='index.html'>
				<h2>Zixian Ma</h2>
			</a>
			<ul class="nav justify-content-center">
				<li class="nav-item">
					<a class=" nav-link" href="asset/pdf/Zixian Ma_Research.pdf">CV</a>
				</li>
				<li class="nav-item">
					<a class=" nav-link" href="#research">Research</a>
				</li>
				<li class="nav-item">
					<a class=" nav-link" href="#project">Project</a>
				</li>
				<!-- <li class="nav-item">
					<a class=" nav-link" href="pages/dance.html">Dance</a>
				</li> -->
			</ul>
		</nav>
	</header>
	<section id="top">
		<div class='container pt-5'>
			<div class="row pt-5">
				<div class="col-4">
					<img class="img-fluid rounded" src="asset/image/horizontal_pic.JPG" alt="headshot">
				</div>
				<div class="col-8">
					<h4 class=''>Ph.D. Student @ University of Washington </h4>
					<!-- <h4 class=''>Computer Science B.S. and M.S. @ Stanford University </h4> -->
					<!-- <h4 class=''>Research Intern @ Google Research </h4> -->
					<div class="row">
						<p class="text-muted mx-3 my-2">zixianma [at] cs.washington.edu</p>
						<span class="icon"><a class="text-dark" href="https://www.linkedin.com/in/zixian-ma/"><i
							class="fab fa-linkedin mx-2"></i></a></span>
				<span class="icon"><a class="text-dark"
						href="https://scholar.google.com/citations?user=0E-IY2IAAAAJ&hl=en&oi=ao"><i
							class="fas fa-graduation-cap mx-2"></i></a></span>
				<span class="icon"><a class="text-dark" href="https://github.com/zixianma"><i
							class="fab fa-github mx-2"></i></a></span>
					</div>
					<p>
						Hi! I am a first-year PhD student at the University of Washington in Computer Science and Engineering supervised by <a class="" href="https://ranjaykrishna.com/index.html">Prof. Ranjay Krishna</a> and <a class="" href="https://allenai.org/team/danw">Prof. Dan Weld</a>. Between Aug 2022 and May 2023, I interned at Google Research and worked at Meta. Before that, I graduated from Stanford University with B.S. (Honors) and M.S. degrees in Computer Science, where I did
						research in the <a class="" href="http://svl.stanford.edu/">Stanford Vision
						and Learning Lab</a> and <a class="" href="https://hci.stanford.edu/ ">HCI group</a>
						under the supervision of the wonderful
						<a class="" href="http://vision.stanford.edu/feifeili/">Prof. Fei-Fei Li</a> and
						<a class="" href="http://hci.stanford.edu/msb/">Prof. Michael Bernstein</a>.
						My current research interests lie broadly in both AI/ML and HCI, especially in multi-modal agents, vision-language models, and human-AI interaction and collaboration. 
					</p>
				</div>
			</div>
			<!-- <div class="row pt-5">
					<p> My past projects cover multi-agent coordination, vision and languague models, and human-computer interaction.
						My current research interests lie in the intersection between AI/ML and HCI, especially human-AI interaction and collaboration. I'm broadly interested in these research questions:<br>
						<ul>
							<li class="bg-light">Human side: what do end users need when collaborating with AI? How can we make explanations helpful (if possible) in human-AI collaboration?</li>
							<li class="bg-light">AI/ML side: how do we develop ML models that can adapt to diverse user needs and behaviors?</li>
							<li class="bg-light">Interaction: what human-AI interactions are possible? How can we leverage interaction data to inform model design and development?</li>
						</ul>
					</p>
			</div> -->
		</div>
	</section>
	<section class="mx-5 my-5" id="highlights">
		<h3 class="mx-5">Recent highlights</h3>
		<ul class=" mx-5">
			<li class="bg-light">
				<b>[Mar, 2024] </b> I'm co-organizing the <a href="https://syndata4cv.github.io/">Synthetic Data for Computer Vision Workshop @ CVPR 2024</a>!
			</li>
			
			<li class="bg-light">
				<b>[Mar, 2024] </b> We've released the dataset and code of <i>m&m's: A Benchmark to Evaluate Tool-Use for multi-step multi-modal Tasks</i>. Check out the <a href="https://mnms-project.github.io/">website</a> for more details!
			</li>
			<li class="bg-light">
				<b>[Sep, 2023] </b> Our paper <i>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</i> has been accepted to NeurIPS 2023 (Datasets & Benchmarks Track)!
			</li>
			<li class="bg-light">
				<b>[Mar, 2023] </b> Our paper <i>CREPE: Can Vision-Language Foundation Models Reason Compositionally?</i> has been accepted to CVPR 2023 and selected as a &#127775;<b>highlight</b>&#127775;!
			</li>
			<li class="bg-light">
				<b>[Jan, 2023] </b> Our paper <i>Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design</i> has been accepted to CHI 2023!
			</li>
			<li class="bg-light">
				<b>[Sep, 2022] </b> Our paper <i>ELIGN: Expectation Alignment as a Multi-agent Intrinsic Reward</i> has been accepted to NeurIPS 2022!
			</li>
			<li class="bg-light">
				<b>[Jun, 2022] </b> I'm grateful that my undergraduate honors thesis <i>Towards More Effective Multi-agent Coordination via Alignment</i>
				has received the Firestone Medal for Excellence in Undergraduate Research and the Ben Wegbreit Prize for 
				Undergraduate CS Research.
			</li>
		</ul>
		</div>
	</section>
	<section class="mx-5 my-5" id="research">
		<h3 class="mx-5">Selected publications</h3>
		<ul class="list-group list-group-flush mx-5">
			<h4 class="mt-3">Preprints</h4>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/mnms_examples.png" alt="m&ms examples">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>m&m's: A Benchmark to Evaluate Tool-Use Agents for multi-step multi-modal Tasks</h4>
						<p><u>Zixian Ma</u>, Weikai Huang, Jieyu Zhang, Tanmay Gupta, Ranjay Krishna</p>
						<p>Arxiv</p>
						<p><a href="https://mnms-project.github.io/">[Website]</a>
							<a href="https://arxiv.org/pdf/2403.11085.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/mnms"> [Code]</a>
						</p>
					</div>
			</li>
		</ul>
		<ul class="list-group list-group-flush mx-5">
			<h4 class="mt-5">Peer-reviewed</h4>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/sugarcrepe.png" alt="sugar crepe image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality</h4>
						<p>Cheng-Yu Hsieh*, Jieyu Zhang*, <u>Zixian Ma</u>, Aniruddha Kembhavi, Ranjay Krishna</p>
						<p>NeurIPS 2023 (Datasets & Benchmarks Track)</p>
						<p><a href="https://arxiv.org/abs/2306.14610.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/sugar-crepe"> [Code]</a>
						</p>
					</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/crepe.png" alt="crepe image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>CREPE: Can Vision-Language Foundation Models Reason Compositionally?</h4>
						<p><u>Zixian Ma*</u>, Jerry Hong*, Mustafa Omer Gul*, Mona Gandhi, Irena Gao, Ranjay Krishna</p>
						<p>CVPR 2023 <span style="color:red;">
							[Highlight: 10% of accepted papers / 2.5% of submissions]</span></p>
						<p><a href="https://arxiv.org/pdf/2212.07796.pdf">[PDF]</a>
							<a href="https://github.com/RAIVNLab/CREPE"> [Code]</a>
						</p>
					</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/model_sketching.png" alt="model sketching image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Model Sketching: Centering Concepts in Early-Stage Machine Learning Model Design</h4>
						<p>Michelle S. Lam, <u>Zixian Ma</u>, Anne Li, Izequiel Freitas, Dakuo Wang, James A. Landay, Michael S. Bernstein</p>
						<p>CHI 2023</p>
						<p><a href="https://arxiv.org/pdf/2303.02884.pdf">[PDF]</a></p>
					</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/align_2.png" alt="ELIGN image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>ELIGN: Expection Alignment as a Multi-agent Intrinsic Reward</h4>
						<p><u>Zixian Ma</u>, Rose Wang, Li Fei-Fei, Michael Bernstein, Ranjay Krishna</p>
						<p>NeurIPS 2022</p>
						<p><a href="https://arxiv.org/pdf/2210.04365.pdf">[PDF]</a><a
								href="https://github.com/StanfordVL/alignment"> [Code]</a></p>
					</div>
			</li>
			<li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/openattack.png" alt="OpenAttack image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Openattack: An Open-source Textual Adversarial Attack Toolkit</h4>
						<p>Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, <u>Zixian Ma</u>, Bairu Hou, Yuan Zang,
							Zhiyuan Liu, Maosong Sun</p>
						<p>ACL2021 Demo</p>
						<p><a href="https://aclanthology.org/2021.acl-demo.43.pdf">[PDF]</a><a
								href="https://github.com/thunlp/OpenAttack"> [Code]</a></p>
					</div>
			</li>
			<!-- <li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/align_1.png" alt="OpenAttack image">
					</div>
					<div class="col-9">
						<h4 id='top' class=''>Towards More Effective Multi-agent Coordination via Alignment</h4>
						<p><u>Zixian Ma</u>, Ranjay Krishna</p>
						<p>Stanford CS Honors Thesis <span style="color:red;">[The Best Honors Thesis]</span></p>
						<p><a href="asset/pdf/Stanford_UG_Honors_Thesis.pdf">[PDF]</a></p>
					</div>
			</li> -->
			<!-- <li class="list-group-item bg-light">
				<div class="row">
					<div class="col-3"><img class="img-fluid" src="asset/image/tert.png" alt="Leap"></div>
					<div class="col-9">
						<h4 id='top' class=''>Telomerase Reverse Transcriptase Promoter Mutations in Hepatocellular
							carcinogenesis</h4>
						<u>Zi-Xian Ma</u>, Chun-Mei Yang, Meng-Ge Li, Hong Tu</p>
						<p>Hepatoma Research</p>
						<p><a
								href="https://oaepublishstorage.blob.core.windows.net/4db8f98f-1917-4ff8-8963-b88ed36f4aa6/3003.pdf">[PDF]</a>
						</p>
					</div>
			</li> -->
		</ul>
		</div>
	</section>
	<section class="mx-5 my-5" id="awards">
		<h3 class="mx-5">Awards</h3>
		<ul class="mx-5">
			<li class="bg-light">
				<a class="" 
				href="https://undergrad.stanford.edu/about-vpue/awards-and-graduation-honors/golden-firestone-and-kennedy-thesis-awards">
				The Firestone Medal for Excellence in Undergraduate Research</a> (Top 10 percent of Honors Theses)
			</li>
			<li class="bg-light">
				The Ben Wegbreit Prize for Undergraduate CS Research (The CS Department's Best Honors Thesis award) 
			</li>
		</ul>
		</div>
	</section>
	<section class="mx-5 my-5" id="project">
		<h3 class="mx-5">Projects</h3>
		<div class="row mx-5">
			<div class="col-4">
				<div class="card mt-3 h-100">
					<img src="asset/image/sexacademy.png" class="card-img-top" alt="...">
					<div class="card-body pb-0">
						<h5 class="card-title">Sex Academy</h5>
						<h6 class="card-subtitle mb-2 text-muted">Personalized sex education with stories.</h6>
						<p class="card-text">Sex Academy is a platform that provides a safe and healthy space for
							students and parents to access customized sex education content recommendations, and peer
							story-sharing of lived experiences.</p>
						<a href="https://web.stanford.edu/class/cs147/projects/EducationTechnology/SexAcademy/"
							class="card-link">Website</a>
						<a href="https://www.figma.com/proto/zmm8pJoijEg7QLCbP10vXf/Sex-Academy-Med-fi-Prototype?node-id=102%3A9700&scaling=scale-down"
							class="card-link">Figma</a>
					</div>
				</div>
			</div>

			<div class="col-4">
				<div class="card mt-3 h-100">
					<img src="asset/image/hungry24.png" class="card-img-top" alt="...">
					<div class="card-body pb-0">
						<h5 class="card-title">Hungry24</h5>
						<h6 class="card-subtitle mb-2 text-muted"></h6>
						<p class="card-text">Hungry24 is a multi-puporse application hosted on Wechat mini-app platform
							that aims to encourage participants to accept a 24-hour hunger challenge. It's a
							collaborative fundraising effort with
							<a href="https://www.cfpa.org.cn/">China Foundation for Poverty Alleviation</a> to alleviate
							starvation for Ethiopian children.
						</p>
						<a href="https://github.com/zixianma/hungry24" class="card-link">Code</a>
						<a href="asset/pdf/hungry_24.pdf" class="card-link">PDF</a>
						<a href="https://mp.weixin.qq.com/s/LwNwhP0-6_4paHag3_qYbw" class="card-link">Article</a>


					</div>
				</div>
			</div>
			<div class="col-4">
				<div class="card mt-3 h-100">
					<img src="asset/image/feedme.png" class="card-img-top" alt="...">
					<div class="card-body pb-0">
						<h5 class="card-title">FeedMe</h5>
						<h6 class="card-subtitle mb-2 text-muted">Auto waste classification based on object detection.
						</h6>
						<p class="card-text">FeedMe is a smart garbage classification and collection device developed at
							the <a href="https://xacademy.cc/index-en.html">X Academy</a> 2018 hackathon.
							It was ranked the second place and won $10K funding.</p>
						<a href="https://github.com/zixianma/Feed-Me" class="card-link">Code</a>
						<a href="asset/pdf/FeedMe_8.15.pdf" class="card-link">PDF</a>
					</div>
				</div>
			</div>
		</div>

	</section>

	<!-- <section class="mx-5 my-5" id="dance">
		<h3 class="mx-5">Dance videos</h3>
		<div class="row mx-5">
			<div class="col-6">
				<div class="ratio ratio-16x9">
					<iframe class="home-vid" src="https://www.youtube.com/embed/MvXeK9cwEVs" title="YouTube video"
						allowfullscreen></iframe>
				</div>
			</div>
			<div class="col-6">
				<div class="ratio ratio-16x9">
					<iframe class="home-vid" src="https://www.youtube.com/embed/2AwFT3Q2caM" title="YouTube video"
						allowfullscreen></iframe>
				</div>
			</div>
		</div>
		<p class="mx-5 d-flex justify-content-end"><a class='' href='pages/dance.html' role='button'>Watch
				more &raquo;</a></p>
		</div>
	</section> -->
	<hr class='featurette-divider'>
	<footer>
		<p class='mx-5 d-flex justify-content-end'><a class="" href='#top'>Back to top</a></p>
	</footer>
</body>

</html>